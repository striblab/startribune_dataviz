Data source: The Minnesota Comprhensive Assessment scores are released by the Minnesota Department of Education, around the end of July each year. They typically provide embargo access to media the day before it is released publicly. You need to apply to MDE each year in order to have access to the embargo site. 
Once the data is public, then you can find the data files in the MDE data center under "assessment and growth."

The files come as tab-delimited, with one for reading, one for math and one for science (which we didn't use this year). There are multiple rows of data for each school, each district, and then county-level and state-level results, too. 

Data for 2012-13, 2013-14, 2014-15 and 2015-16 are stored in sandbox2 on the stsqlnewsusr server. They are in one table called "schools_MCA";
there are tables that I created, called schools_schoollist and schools_districtlist, that are lookup tables with one record for each school (or district);
this is where I've added some additional info that we find useful, such as whether the school/district is in the 7-county metro and things like that; Each year
those tables need to be updated to add new schools/district. Would also be useful going forward to flag any schools/districts that are in those lists that are no longer operating.

On the virtual server, stncar, I have a "Schools" Directory where I've saved the import specs for bringing in the test score files and the "special enrollment" population
files that MDE puts out each year (these files have the number of kids on free-reduced lunch)
There is also a "field descriptions" file from MDE for the test score data.

Next year:
Before the new data arrives, be sure to get updated files on special population enrollment from MDE and import those to the "schools_specialenroll" table
on stsqlnewsusr in sandbox2. You can also use this to figure out which schools/districts need to be added to the schoollist and districtlist tables. Also check to 
for any name changes.

Look at the saved queries in "queries for test score analysis.sql" (on stncar) to see if they will be sufficient for the schools reporters. Most of the queries 
in there are meant to generate data that reporters can use for their story. Adjust as needed.

Go back into the code and create a uniqueID for all records made up of:   schoolID+"-"+datayear+"-"+subject.

When new MCA data arrives:
* Use the Visual Studio import solution to put the math and reading files into schools_MCA table. 
* Use the saved queries in "queries for test score analysis.sql" (on stncar) to run some update queries (populating schoolID is crucial first step) 
and pull out whatever data the reporters need for their analysis.
* At the bottom of those queries is the code for pulling out data for the data visualization. This assumes that the data viz will run off the same fields as we did for 15-16.
* That data next needs to get run through SPSS linear regression. (see notes below)

WATCH OUT FOR:
* The file that goes to the data viz needs to be adjusted a bit before sending over. The datayear field and the grade range field proved especially
problematic because Excel or Google Sheets kept converting "15-16" to a date. Or the grade range ones that said "05-06" or something like that got converted too. So I did a
replace all and swapped out the dashes (-) for the word "to" in both fields.

* SPSS cut off variables that were too long (i.e. school name and district name); this may have been a problem on my end with the import process
in SPSS. It's probably possible to make sure those fields are set wide enough when you bring in the data, so that they are intact. But 
I didn't catch that this year, so I ended up having to marry the regression results and the original file exported from SQL SErver. I married them
by first creating unique IDs in each file. The Unique ID was made up of the schoolID+"-"+datayear+"-"+subject. Then I joined on that. 

SPSS REGRESSION:
The data needs to include percent proficient and percent on free/reduced lunch (pctpoverty) and counttested and subject, plus schoolID and datayear at minimum. 

Setting up: Filter cases to only those where at least 25 students were tested

Run separate regressions -- one where filtered to math as the subject; one filtered to where reading is the subject; Have them spit out 
unstandardized scores and residuals; it will create one set of fields for the math output and one for reading; will need to 
merge them together before sending data for data viz

After running regression, add fields to the file for the data viz:


Field to identify the category:
Better than expected -- residual is 10 or more percentage points (positive)
Falling short -- residual is 10 or more percentage points (negative)
As expected -- everything in between

I also created a field that numbers those categories:
0 - Not enough students tested
1 - Falling short
2 - As expected
3 - Better than expected


DATA VIZ IDEA CHANGES:
* Would like to be able to filter the list by the poverty level (high, medium, low), so that users could easily find the high-poverty schools
that are "beating the odds" and the low-poverty schools that are "falling short"

The print "beating the odds" and "falling short" lists -- were very popular. The beating odds list was only high-poverty metro-area schools and
we pulled out the 10 with the biggest residual; the falling short was the 10 metro-area, low-poverty schools with biggest negative residuals; These are
certainly incomplete "beating the odds" lists and I'd like to find a way to be able to allow users to see ALL the high-poverty schools 
making that 

* would like to revisit UX issues; we were worried that users would have a hard time understanding that the percentages at the top were also
buttons for filtering. I didn't hear any complaints, though





